{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## api로 데이터 정보 가져오기\n","- Channel_ID: 채널 ID\n","-Channel_Title: 채널 이름\n","-Video_ID: 동영상 ID\n","-Title: 동영상 제목\n","-Upload_Date: 업로드 날짜\n","-Comment_ID: 댓글 ID\n","-Comment_Author: 댓글 작성자 이름\n","-Is_Channel_Author: 댓글 작성자가 채널 주인인지 여부\n","-Comment_Published_At: 댓글 작성 날짜\n","-Is_Short: 숏 비디오 여부"],"metadata":{"id":"023CvTqURL1a"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-AjXZm7PnPE","executionInfo":{"status":"error","timestamp":1721885696708,"user_tz":-540,"elapsed":2224,"user":{"displayName":"박현진","userId":"02603468693043301355"}},"outputId":"700500de-e814-4759-eb7b-db5460a1fef9","colab":{"base_uri":"https://localhost:8080/","height":402}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'isodate'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-533c7ae53549>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoogleapiclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0misodate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogleapiclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'isodate'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import googleapiclient.discovery\n","import pandas as pd\n","import isodate\n","from tqdm import tqdm\n","from googleapiclient.errors import HttpError\n","\n","# YouTube API 서비스를 생성하는 함수\n","def get_youtube_service(api_key):\n","    return googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key)\n","\n","# 특정 비디오의 정보를 가져오는 함수\n","def get_video_info(youtube, video_id):\n","    try:\n","        request = youtube.videos().list(\n","            part='snippet,contentDetails',\n","            id=video_id,\n","            fields='items(id,snippet(title,channelId,channelTitle,publishedAt),contentDetails(duration))'\n","        )\n","        response = request.execute()\n","        return response['items'][0]\n","    except Exception as e:\n","        print(f\"Error fetching video info for {video_id}: {e}\")\n","        return None\n","\n","# 특정 채널의 정보를 가져오는 함수\n","def get_channel_info(youtube, channel_id):\n","    try:\n","        request = youtube.channels().list(\n","            part='snippet,statistics',\n","            id=channel_id,\n","            fields='items(statistics(subscriberCount))'\n","        )\n","        response = request.execute()\n","        return response['items'][0]['statistics']\n","    except Exception as e:\n","        print(f\"Error fetching channel info for {channel_id}: {e}\")\n","        return None\n","\n","# 채널 이름을 기반으로 채널 ID를 가져오는 함수\n","def get_channel_id_by_name(youtube, channel_name):\n","    try:\n","        request = youtube.search().list(\n","            part='snippet',\n","            q=channel_name,\n","            type='channel',\n","            fields='items(id(channelId),snippet(channelTitle))'\n","        )\n","        response = request.execute()\n","        return response['items'][0]['id']['channelId']\n","    except Exception as e:\n","        print(f\"Error fetching channel ID for {channel_name}: {e}\")\n","        return None\n","\n","# 특정 채널의 비디오 ID를 가져오는 함수\n","def get_video_ids_from_channel(youtube, channel_id):\n","    video_ids = []\n","    page_token = None\n","    while True:\n","        try:\n","            request = youtube.search().list(\n","                part='id',\n","                channelId=channel_id,\n","                maxResults=50,\n","                pageToken=page_token,\n","                type='video'\n","            )\n","            response = request.execute()\n","            video_ids += [item['id']['videoId'] for item in response['items']]\n","            if 'nextPageToken' in response:\n","                page_token = response['nextPageToken']\n","            else:\n","                break\n","        except HttpError as e:\n","            print(f\"Error fetching video IDs for channel {channel_id}: {e}\")\n","            break\n","        except Exception as e:\n","            print(f\"Error fetching video IDs for channel {channel_id}: {e}\")\n","            break\n","    return video_ids\n","\n","# 동영상의 댓글 정보를 가져오는 함수 (페이지네이션 적용)\n","def get_all_comments(youtube, video_id):\n","    comments = []\n","    page_token = None\n","\n","    while True:\n","        try:\n","            request = youtube.commentThreads().list(\n","                part='snippet,replies',\n","                videoId=video_id,\n","                maxResults=100,\n","                pageToken=page_token\n","            )\n","            response = request.execute()\n","\n","            for item in response['items']:\n","                try:\n","                    top_comment = item['snippet']['topLevelComment']['snippet']\n","                    comment = {\n","                        'comment_thread_id': item['id'],\n","                        'video_id': video_id,\n","                        'author_display_name': top_comment['authorDisplayName'],\n","                        'published_at': format_datetime(top_comment['publishedAt']),\n","                        'author_channel_id': top_comment['authorChannelId']['value'] if 'authorChannelId' in top_comment else None\n","                    }\n","                    comments.append(comment)\n","\n","                    if 'replies' in item:\n","                        reply_count = item['snippet']['totalReplyCount']\n","                        if reply_count > 0:\n","                            reply_page_token = None\n","                            while True:\n","                                try:\n","                                    reply_request = youtube.comments().list(\n","                                        part='snippet',\n","                                        parentId=item['id'],\n","                                        maxResults=min(reply_count, 100),\n","                                        pageToken=reply_page_token\n","                                    )\n","                                    reply_response = reply_request.execute()\n","\n","                                    for reply_item in reply_response['items']:\n","                                        try:\n","                                            reply_comment = {\n","                                                'comment_thread_id': reply_item['id'],\n","                                                'video_id': video_id,\n","                                                'author_display_name': reply_item['snippet']['authorDisplayName'],\n","                                                'published_at': format_datetime(reply_item['snippet']['publishedAt']),\n","                                                'author_channel_id': reply_item['snippet']['authorChannelId']['value'] if 'authorChannelId' in reply_item['snippet'] else None\n","                                            }\n","                                            comments.append(reply_comment)\n","                                        except KeyError as e:\n","                                            print(f\"KeyError in processing reply: {e}\")\n","\n","                                    if 'nextPageToken' in reply_response:\n","                                        reply_page_token = reply_response['nextPageToken']\n","                                    else:\n","                                        break\n","\n","                                except HttpError as e:\n","                                    print(f\"Error fetching replies for comment {item['id']}: {e}\")\n","                                    break\n","\n","                                except Exception as e:\n","                                    print(f\"Error fetching replies for comment {item['id']}: {e}\")\n","                                    break\n","\n","                except KeyError as e:\n","                    print(f\"KeyError in processing comment: {e}\")\n","\n","            if 'nextPageToken' in response:\n","                page_token = response['nextPageToken']\n","            else:\n","                break\n","\n","        except HttpError as e:\n","            print(f\"Error fetching comments for video {video_id}: {e}\")\n","            break\n","\n","        except Exception as e:\n","            print(f\"Error fetching comments for video {video_id}: {e}\")\n","            break\n","\n","    return comments\n","\n","# 동영상 길이를 확인하여 short 비디오인지 여부를 판단하는 함수\n","def is_short(video_duration):\n","    try:\n","        duration = isodate.parse_duration(video_duration)\n","        return duration.total_seconds() <= 60\n","    except Exception as e:\n","        print(f\"Error parsing duration: {e}\")\n","        return False\n","\n","# 날짜 및 시간을 지정된 형식으로 변환하는 함수\n","def format_datetime(datetime_str):\n","    try:\n","        return pd.to_datetime(datetime_str).strftime('%Y-%m-%dT%H:%M')\n","    except Exception as e:\n","        print(f\"Error formatting datetime: {e}\")\n","        return datetime_str\n","\n","# 데이터 수집 함수\n","def collect_data(youtube, video_ids, start_index, end_index):\n","    combined_data = []\n","    selected_video_ids = video_ids[start_index:end_index]\n","\n","    for video_id in tqdm(selected_video_ids, desc=\"Processing videos\"):\n","        video_info = get_video_info(youtube, video_id)\n","        if video_info:\n","            channel_id = video_info['snippet']['channelId']\n","            channel_author = video_info['snippet']['channelTitle']\n","            title = video_info['snippet']['title']\n","            duration = video_info['contentDetails']['duration']\n","            upload_date = format_datetime(video_info['snippet']['publishedAt'])\n","\n","            # 동영상 기본 정보 추가\n","            combined_data.append({\n","                'Channel_ID': channel_id,\n","                'Channel_Title': channel_author,\n","                'Video_ID': video_id,\n","                'Title': title,\n","                'Upload_Date': upload_date,\n","                'Comment_ID': None,\n","                'Comment_Author': None,\n","                'Is_Channel_Author': True,\n","                'Comment_Published_At': None,\n","                'Is_Short': is_short(duration)\n","            })\n","\n","            # 댓글 데이터를 추가\n","            comments = get_all_comments(youtube, video_id)\n","            for comment in comments:\n","                combined_data.append({\n","                    'Channel_ID': channel_id,\n","                    'Channel_Title': channel_author,\n","                    'Video_ID': video_id,\n","                    'Title': title,\n","                    'Upload_Date': upload_date,\n","                    'Comment_ID': comment['comment_thread_id'],\n","                    'Comment_Author': comment['author_display_name'],\n","                    'Is_Channel_Author': (comment['author_channel_id'] == channel_id),\n","                    'Comment_Published_At': comment['published_at'],\n","                    'Is_Short': is_short(duration)\n","                })\n","\n","    return combined_data\n","\n","# 데이터 저장 함수\n","def save_data(data, file_name):\n","    try:\n","        df_combined = pd.DataFrame(data)\n","        df_combined.to_csv(f'{file_name}.csv', index=False)\n","        print(\"데이터를 저장했습니다.\")\n","    except Exception as e:\n","        print(f\"Error saving data: {e}\")\n","\n","# 메인 함수\n","def main():\n","    api_key = input(\"API 키를 입력하세요 (종료하려면 'done' 입력): \")\n","    if api_key.lower() == 'done':\n","        print(\"프로그램을 종료합니다.\")\n","        return\n","\n","    youtube = get_youtube_service(api_key)\n","\n","    # 채널명을 입력받아 해당 채널의 비디오 ID를 가져옵니다.\n","    channel_name = input(\"채널명을 입력하세요: \")\n","    channel_id = get_channel_id_by_name(youtube, channel_name)\n","    if not channel_id:\n","        print(f\"채널 '{channel_name}'을 찾을 수 없습니다.\")\n","        return\n","\n","    video_ids = get_video_ids_from_channel(youtube, channel_id)\n","    print(f\"채널 '{channel_name}'에서 {len(video_ids)}개의 비디오를 찾았습니다.\")\n","\n","    start_index = int(input(\"비디오 ID를 가져올 시작 인덱스를 입력하세요 (0부터 시작): \"))\n","    end_index = int(input(\"비디오 ID를 가져올 끝 인덱스를 입력하세요 (마지막 비디오 ID는 포함되지 않음): \"))\n","\n","    file_name = input(\"저장할 파일명을 입력하세요 (확장자 제외): \")\n","\n","    if not file_name:\n","        print(\"파일명은 필수 입력 사항입니다.\")\n","        return\n","\n","    data = collect_data(youtube, video_ids, start_index, end_index)\n","    save_data(data, file_name)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","source":["## 전처리(날짜 기준 통합)"],"metadata":{"id":"-2_xmbgRWLRa"}},{"cell_type":"code","source":[],"metadata":{"id":"pJq8l98YWGj8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 전처리(쇼츠와 영상을 나눔)"],"metadata":{"id":"2t6H9NUFRsEO"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# 원본 데이터프레임을 복사하여 사용합니다.\n","data =pd.read_csv(r\"자신이 가져온 데이터 경로\")\n","\n","# 필요한 컬럼만 추출합니다.\n","columns_to_use = [\n","    'Channel_ID', 'Channel_Title',\n","    'Video_ID', 'Title', 'Upload_Date', 'Is_Short',\n","    'Comment_ID', 'Comment_Published_At', 'Comment_Author', 'Is_Channel_Author'\n","]\n","data = data[columns_to_use]\n","\n","# 'Is_Short' 값에 따라 데이터프레임을 나누어 복사합니다.\n","data1 = data[data['Is_Short'] == True].copy()\n","data2 = data[data['Is_Short'] == False].copy()\n","\n","def processing1(data):\n","    # 'Upload_Date'와 'Comment_Published_At'을 datetime 형식으로 변환합니다.\n","    data['Upload_Date'] = pd.to_datetime(data['Upload_Date'])\n","    data['Comment_Published_At'] = pd.to_datetime(data['Comment_Published_At'])\n","\n","    # 댓글 ID 기준 결측치를 제거합니다.\n","    data = data.dropna(subset=['Comment_ID'])\n","\n","    # 댓글 ID 기준 중복을 제거합니다.\n","    data = data.drop_duplicates(subset=['Comment_ID'])\n","\n","    # 채널 주인의 댓글을 제외합니다.\n","    data_filtered = data[data['Is_Channel_Author'] != 1]\n","\n","    # 영상별로 동일한 작성자의 댓글을 1개씩만 남깁니다.\n","    data = data_filtered.drop_duplicates(subset=['Video_ID', 'Comment_Author'])\n","\n","    # 인덱스를 리셋합니다.\n","    data = data.reset_index(drop=True)\n","\n","    # 남은 댓글 ID의 개수를 확인합니다.\n","    remaining_comment_count = data['Comment_ID'].nunique()\n","    print(f\"채널 주인의 댓글을 제외하고, 영상별로 작성자의 댓글을 1개씩만 남긴 후의 댓글 ID 개수는 {remaining_comment_count}개 입니다.\")\n","\n","        # 필요한 컬럼만 추출\n","    columns_to_use = [\n","        'Channel_ID', 'Channel_Title', 'Upload_Date', 'Comment_Published_At',\n","        'Video_ID', 'Comment_ID'\n","    ]\n","    data = data[columns_to_use].copy()  # 명시적으로 사본을 만듭니다.\n","\n","    # 날짜 형식에서 년-월-일 정보만 추출\n","    data['Upload_Date'] = data['Upload_Date'].astype(str).str[:10]\n","    data['Comment_Published_At'] = data['Comment_Published_At'].astype(str).str[:10]\n","\n","    # 'Upload_Date'와 'Comment_Published_At'을 datetime 형식으로 변환 (년, 월, 일까지만)\n","    data['Upload_Date'] = pd.to_datetime(data['Upload_Date'], format='%Y-%m-%d', errors='coerce')\n","    data['Comment_Published_At'] = pd.to_datetime(data['Comment_Published_At'], format='%Y-%m-%d', errors='coerce')\n","\n","    # 결과를 저장할 데이터프레임 생성\n","    df = pd.DataFrame()\n","\n","    # 각 채널별로 처리\n","    try:\n","        for channel_id, channel_data in data.groupby('Channel_ID'):\n","            # 첫 업로드 날짜 찾기\n","            first_upload_date = channel_data['Upload_Date'].min()\n","\n","            # 첫 업로드 날짜부터 현재까지의 월요일 기준 주 단위 날짜 생성\n","            date_range = pd.date_range(start=first_upload_date, end=channel_data['Upload_Date'].max() + pd.Timedelta(days=7), freq='W-MON')\n","\n","            # 주간 데이터 저장 리스트 초기화\n","            weekly_data = []\n","\n","            # 누적 값 초기화\n","            cumulative_videos = 0\n","            cumulative_comments = 0\n","\n","            for i, week_start in enumerate(date_range):\n","                week_end = week_start + pd.Timedelta(days=7)\n","\n","                # 현재 주의 비디오와 댓글 필터링\n","                weekly_videos = channel_data[(channel_data['Upload_Date'] >= week_start) & (channel_data['Upload_Date'] < week_end)]\n","                weekly_comments = channel_data[(channel_data['Comment_Published_At'] >= week_start) & (channel_data['Comment_Published_At'] < week_end)]\n","\n","                # 고유 Comment_ID와 Comment_Published_At을 사용하여 누적 값 업데이트\n","                unique_comments = weekly_comments.drop_duplicates(subset=['Comment_ID', 'Comment_Published_At']).shape[0]\n","                cumulative_videos += weekly_videos['Video_ID'].nunique()\n","                cumulative_comments += unique_comments\n","\n","                # 주간 데이터 추가\n","                weekly_data.append({\n","                    'Channel_ID': channel_id,\n","                    'Channel_Title': channel_data['Channel_Title'].iloc[0],\n","                    'Comment_Year': week_start.year,\n","                    'Comment_Week': week_start.isocalendar()[1],\n","                    'Cumulative_Videos': int(cumulative_videos),\n","                    'Cumulative_Comments': int(cumulative_comments),\n","                })\n","\n","            # 현재 채널의 결과 데이터프레임 생성\n","            channel_df = pd.DataFrame(weekly_data)\n","\n","            # 결과에 추가\n","            df = pd.concat([df, channel_df], ignore_index=True)\n","    except Exception as e:\n","        print(f\"데이터 처리 중 오류 발생: {e}\")\n","\n","    # 'Cumulative_Videos'가 0인 행 제거\n","    if 'Cumulative_Videos' in df.columns:\n","        df = df[(df['Cumulative_Videos'] != 0)]\n","    else:\n","        print(\"'Cumulative_Videos' 열이 결과 데이터프레임에 존재하지 않습니다.\")\n","\n","    # 열 순서 정리\n","    expected_columns = ['Channel_ID', 'Channel_Title', 'Comment_Year', 'Comment_Week', 'Cumulative_Videos', 'Cumulative_Comments']\n","    missing_columns = [col for col in expected_columns if col not in df.columns]\n","    if missing_columns:\n","        print(f\"다음 열이 결과 데이터프레임에 없습니다: {missing_columns}\")\n","    else:\n","        df = df[expected_columns]\n","\n","    # 누적 댓글 수를 누적 영상 수로 나누기\n","    df['Comments_Per_Video'] = df['Cumulative_Comments'] / df['Cumulative_Videos']\n","    df['Year_Week'] = df['Comment_Year'].astype(str) + '-' + df['Comment_Week'].astype(str)\n","    df = df.drop(['Channel_ID','Comment_Year','Comment_Week','Cumulative_Videos','Cumulative_Comments'],axis=1)\n","\n","    return df\n","\n","# 각 데이터프레임을 처리하고 결과를 저장합니다.\n","data1 = processing1(data1)\n","data2 = processing1(data2)\n"],"metadata":{"id":"qMwNMoGPSOXi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 영상 주간 각도가 가장 높은 시점이 90도가 넘는 다면--떡상"],"metadata":{"id":"ahTs5qUnSchX"}},{"cell_type":"code","source":["import pandas as pd\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","\n","# 슬라이딩 윈도우와 기울기 계산 함수\n","def calculate_slopes(data, periods):\n","    if not isinstance(periods, int):\n","        raise ValueError(\"`periods` must be an integer.\")\n","\n","    slopes = []\n","    channels = data['Channel_Title'].unique()\n","\n","    # 단일 기간만 처리하는 경우\n","    for period in tqdm([periods], desc='Creating sliding windows and calculating slopes'):\n","        for channel in channels:\n","            channel_data = data[data['Channel_Title'] == channel]\n","            if len(channel_data) >= period:\n","                for i in range(len(channel_data) - period + 1):\n","                    window = channel_data.iloc[i:i+period]\n","                    comments_per_video = window['Comments_Per_Video'].values\n","                    year_week = window['Year_Week'].values[-1]  # 마지막 주차를 기간으로 사용\n","\n","                    # x축 값 생성 (주차를 기준으로 0부터 시작)\n","                    x = np.arange(len(comments_per_video)).reshape(-1, 1)\n","                    y = comments_per_video\n","\n","                    # 선형 회귀 모델을 사용하여 기울기 계산\n","                    model = LinearRegression().fit(x, y)\n","                    slope = model.coef_[0]  # 기울기\n","\n","                    slopes.append({\n","                        'Channel_Title': channel,\n","                        'Period': period,\n","                        'Year_Week': year_week,\n","                        'Window': comments_per_video,\n","                        'Slope': slope\n","                    })\n","        print(f\"Period {period}: {len(slopes)} slopes calculated\")\n","\n","    return pd.DataFrame(slopes)\n","\n","# 채널 이름 리스트\n","Channel_name = data1_processed['Channel_Title'].unique()\n","\n","# 기울기를 딕셔너리에 저장\n","slopes_dict = {}\n","slopes_list = []\n","\n","def process_data(df, video_type):\n","    for channel in Channel_name:\n","        channel_data = df[df['Channel_Title'] == channel]\n","        max_slope_row = channel_data[channel_data['Slope'] == channel_data['Slope'].max()]\n","        max_slope = max_slope_row['Slope'].values[0]\n","        max_year_week = max_slope_row['Year_Week'].values[0]\n","        slopes_dict[channel] = (max_slope, max_year_week)\n","        slopes_list.append(max_slope)\n","\n","        print(f'{video_type}의 경우')\n","        print(f'{channel}의 가장 큰 기울기: {max_slope} (기간: {max_year_week})')\n","\n","        if max_slope > 90:\n","            print(f'--> {channel}는 떡상 기준에 적합합니다! (기울기: {max_slope} > 90도)')\n","            print('-------------------------------')\n","\n","# 데이터 전처리된 데이터프레임을 사용하여 슬라이딩 윈도우와 기울기 계산\n","period = 2  # 슬라이딩 윈도우의 기간 (예시로 4주 설정)\n","data1_slopes = calculate_slopes(data1_processed, period)\n","data2_slopes = calculate_slopes(data2_processed, period)\n","\n","# 긴 동영상과 쇼츠를 각각 처리\n","process_data(data1_slopes, \"쇼츠\")\n","process_data(data2_slopes, \"긴 동영상\")\n"],"metadata":{"id":"e7c0ZGuhWSir"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 영상의 전체 기간 중에 최근 25%에 해당하는 기간에서 0.5% 차이가 나는 영상이 30%가 넘으면 하락중 아니면 유지중"],"metadata":{"id":"IfrieD5EZfNb"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import font_manager, rc\n","\n","def analyze_channel1(df, channel_name):\n","    # 한글 폰트 설정\n","    font_path = 'C:/Windows/Fonts/malgun.ttf'  # 윈도우의 경우\n","    font_name = font_manager.FontProperties(fname=font_path).get_name()\n","    rc('font', family=font_name)\n","\n","    # 채널에 해당하는 데이터 필터링 및 복사본 생성\n","    df1 = df[df.Channel_Title == channel_name].copy()  # 원본 데이터프레임의 복사본을 사용\n","    df1 = df1.reset_index(drop=True)\n","\n","    # 행이 5개 이하인 경우 메시지 출력\n","    if len(df1) <= 5:\n","        print(f\"{channel_name} 채널은 아직 분석에 대한 데이터가 쌓이지 않았습니다.\")\n","        return\n","\n","    # 시계열 데이터 준비\n","    signal = df1['Comments_Per_Video']\n","\n","    # 이동 평균 계산\n","    df1['4_Week_Moving_Avg'] = signal.rolling(window=4).mean()\n","    df1['Weekly_Moving_Avg'] = signal.rolling(window=1).mean()  # 주당 이동 평균\n","    df1['3_Week_Moving_Avg'] = signal.rolling(window=3).mean()  # 3주 이동 평균\n","\n","    # 변화율 계산 (0인 값은 제외)\n","    df1['Pct_Change'] = signal.replace(0, np.nan).pct_change().replace(np.nan, 0)\n","\n","    # 모든 데이터를 포함한 필터링\n","    df1_filtered = df1.copy()\n","\n","    # 주평선과 3주평선의 차이 계산\n","    df1_filtered['Moving_Avg_Diff'] = df1_filtered['Weekly_Moving_Avg'] - df1_filtered['3_Week_Moving_Avg']\n","\n","    # 주평선과 3주평선의 값이 10 이상인 경우에만 차이가 10% 이하로 나는 구간 찾기\n","    significant_diff_points = df1_filtered[\n","        (df1_filtered['Weekly_Moving_Avg'] > 10) &\n","        (df1_filtered['3_Week_Moving_Avg'] > 10) &\n","        (df1_filtered['Moving_Avg_Diff'] < df1_filtered['3_Week_Moving_Avg'] * -0.005)\n","    ]\n","\n","    # 최근 25% 기간 추출\n","    num_periods = len(df1_filtered)\n","    recent_periods = int(num_periods * 0.25)\n","    recent_data = df1_filtered.tail(recent_periods)\n","\n","    # 최근 25% 기간 내에서 기준을 충족하는 구간 비율 계산\n","    recent_significant_diff_points = recent_data[\n","        (recent_data['Weekly_Moving_Avg'] > 10) &\n","        (recent_data['3_Week_Moving_Avg'] > 10) &\n","        (recent_data['Moving_Avg_Diff'] < recent_data['3_Week_Moving_Avg'] * -0.005)\n","    ]\n","\n","    if len(recent_significant_diff_points) / recent_periods > 0.30:\n","        trend_status = \"하락 중\"\n","    else:\n","        trend_status = \"유지 중\"\n","\n","    # 분석 결과 시각화\n","    plt.figure(figsize=(12, 6))\n","\n","    # 원본 데이터와 이동 평균 시각화\n","    plt.plot(df1_filtered['Year_Week'], df1_filtered['Comments_Per_Video'], label='Comments Per Video', marker='o')\n","    plt.plot(df1_filtered['Year_Week'], df1_filtered['4_Week_Moving_Avg'], label='4-Week Moving Average', linestyle='--')\n","    plt.plot(df1_filtered['Year_Week'], df1_filtered['Weekly_Moving_Avg'], label='Weekly Moving Average', linestyle='-.')\n","    plt.plot(df1_filtered['Year_Week'], df1_filtered['3_Week_Moving_Avg'], label='3-Week Moving Average', linestyle=':')\n","\n","    # 차이가 10% 이하로 나는 구간 시각화\n","    if not significant_diff_points.empty:\n","        plt.plot(significant_diff_points['Year_Week'], significant_diff_points['Comments_Per_Video'], 'ro', label='Significant Difference (<10%)')\n","\n","    plt.xlabel('Year-Week')\n","    plt.ylabel('Comments Per Video')\n","    plt.title(f'{channel_name}: Comments Per Video Analysis')\n","    plt.xticks(rotation=90)\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","    # 변화율 요약 통계\n","    pct_change_summary = df1_filtered['Pct_Change'].describe()\n","\n","    print(f'{channel_name} 변화율 요약 통계:')\n","    print(pct_change_summary)\n","\n","    # 최근 25% 기간의 기준 충족 비율에 따라 상태 출력\n","    print(f'{channel_name}의 최근 25% 기간 상태: {trend_status}')\n","\n","# 채널 이름 리스트\n","Channel_name = data1_processed['Channel_Title'].unique()\n","\n","for df in [data1_processed, data2_processed]:\n","    for channel in Channel_name:\n","        analyze_channel1(df, channel)\n"],"metadata":{"id":"-g0h6gN5Zfth"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 최종합본"],"metadata":{"id":"qw3yrnuFfbf-"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import font_manager, rc\n","from tqdm import tqdm\n","from sklearn.linear_model import LinearRegression\n","import googleapiclient.discovery\n","import isodate\n","from tqdm import tqdm\n","from googleapiclient.errors import HttpError\n","\n","# YouTube API 서비스를 생성하는 함수\n","def get_youtube_service(api_key):\n","    return googleapiclient.discovery.build('youtube', 'v3', developerKey=api_key)\n","\n","# 특정 비디오의 정보를 가져오는 함수\n","def get_video_info(youtube, video_id):\n","    try:\n","        request = youtube.videos().list(\n","            part='snippet,contentDetails',\n","            id=video_id,\n","            fields='items(id,snippet(title,channelId,channelTitle,publishedAt),contentDetails(duration))'\n","        )\n","        response = request.execute()\n","        return response['items'][0]\n","    except Exception as e:\n","        print(f\"Error fetching video info for {video_id}: {e}\")\n","        return None\n","\n","# 특정 채널의 정보를 가져오는 함수\n","def get_channel_info(youtube, channel_id):\n","    try:\n","        request = youtube.channels().list(\n","            part='snippet,statistics',\n","            id=channel_id,\n","            fields='items(statistics(subscriberCount))'\n","        )\n","        response = request.execute()\n","        return response['items'][0]['statistics']\n","    except Exception as e:\n","        print(f\"Error fetching channel info for {channel_id}: {e}\")\n","        return None\n","\n","# 채널 이름을 기반으로 채널 ID를 가져오는 함수\n","def get_channel_id_by_name(youtube, channel_name):\n","    try:\n","        request = youtube.search().list(\n","            part='snippet',\n","            q=channel_name,\n","            type='channel',\n","            fields='items(id(channelId),snippet(channelTitle))'\n","        )\n","        response = request.execute()\n","        return response['items'][0]['id']['channelId']\n","    except Exception as e:\n","        print(f\"Error fetching channel ID for {channel_name}: {e}\")\n","        return None\n","\n","# 특정 채널의 비디오 ID를 가져오는 함수\n","def get_video_ids_from_channel(youtube, channel_id):\n","    video_ids = []\n","    page_token = None\n","    while True:\n","        try:\n","            request = youtube.search().list(\n","                part='id',\n","                channelId=channel_id,\n","                maxResults=50,\n","                pageToken=page_token,\n","                type='video'\n","            )\n","            response = request.execute()\n","            video_ids += [item['id']['videoId'] for item in response['items']]\n","            if 'nextPageToken' in response:\n","                page_token = response['nextPageToken']\n","            else:\n","                break\n","        except HttpError as e:\n","            print(f\"Error fetching video IDs for channel {channel_id}: {e}\")\n","            break\n","        except Exception as e:\n","            print(f\"Error fetching video IDs for channel {channel_id}: {e}\")\n","            break\n","    return video_ids\n","\n","# 동영상의 댓글 정보를 가져오는 함수 (페이지네이션 적용)\n","def get_all_comments(youtube, video_id):\n","    comments = []\n","    page_token = None\n","\n","    while True:\n","        try:\n","            request = youtube.commentThreads().list(\n","                part='snippet,replies',\n","                videoId=video_id,\n","                maxResults=100,\n","                pageToken=page_token\n","            )\n","            response = request.execute()\n","\n","            for item in response['items']:\n","                try:\n","                    top_comment = item['snippet']['topLevelComment']['snippet']\n","                    comment = {\n","                        'comment_thread_id': item['id'],\n","                        'video_id': video_id,\n","                        'author_display_name': top_comment['authorDisplayName'],\n","                        'published_at': format_datetime(top_comment['publishedAt']),\n","                        'author_channel_id': top_comment['authorChannelId']['value'] if 'authorChannelId' in top_comment else None\n","                    }\n","                    comments.append(comment)\n","\n","                    if 'replies' in item:\n","                        reply_count = item['snippet']['totalReplyCount']\n","                        if reply_count > 0:\n","                            reply_page_token = None\n","                            while True:\n","                                try:\n","                                    reply_request = youtube.comments().list(\n","                                        part='snippet',\n","                                        parentId=item['id'],\n","                                        maxResults=min(reply_count, 100),\n","                                        pageToken=reply_page_token\n","                                    )\n","                                    reply_response = reply_request.execute()\n","\n","                                    for reply_item in reply_response['items']:\n","                                        try:\n","                                            reply_comment = {\n","                                                'comment_thread_id': reply_item['id'],\n","                                                'video_id': video_id,\n","                                                'author_display_name': reply_item['snippet']['authorDisplayName'],\n","                                                'published_at': format_datetime(reply_item['snippet']['publishedAt']),\n","                                                'author_channel_id': reply_item['snippet']['authorChannelId']['value'] if 'authorChannelId' in reply_item['snippet'] else None\n","                                            }\n","                                            comments.append(reply_comment)\n","                                        except KeyError as e:\n","                                            print(f\"KeyError in processing reply: {e}\")\n","\n","                                    if 'nextPageToken' in reply_response:\n","                                        reply_page_token = reply_response['nextPageToken']\n","                                    else:\n","                                        break\n","\n","                                except HttpError as e:\n","                                    print(f\"Error fetching replies for comment {item['id']}: {e}\")\n","                                    break\n","\n","                                except Exception as e:\n","                                    print(f\"Error fetching replies for comment {item['id']}: {e}\")\n","                                    break\n","\n","                except KeyError as e:\n","                    print(f\"KeyError in processing comment: {e}\")\n","\n","            if 'nextPageToken' in response:\n","                page_token = response['nextPageToken']\n","            else:\n","                break\n","\n","        except HttpError as e:\n","            print(f\"Error fetching comments for video {video_id}: {e}\")\n","            break\n","\n","        except Exception as e:\n","            print(f\"Error fetching comments for video {video_id}: {e}\")\n","            break\n","\n","    return comments\n","\n","# 동영상 길이를 확인하여 short 비디오인지 여부를 판단하는 함수\n","def is_short(video_duration):\n","    try:\n","        duration = isodate.parse_duration(video_duration)\n","        return duration.total_seconds() <= 60\n","    except Exception as e:\n","        print(f\"Error parsing duration: {e}\")\n","        return False\n","\n","# 날짜 및 시간을 지정된 형식으로 변환하는 함수\n","def format_datetime(datetime_str):\n","    try:\n","        return pd.to_datetime(datetime_str).strftime('%Y-%m-%dT%H:%M')\n","    except Exception as e:\n","        print(f\"Error formatting datetime: {e}\")\n","        return datetime_str\n","\n","# 데이터 수집 함수\n","def collect_data(youtube, video_ids, start_index, end_index):\n","    combined_data = []\n","    selected_video_ids = video_ids[start_index:end_index]\n","\n","    for video_id in tqdm(selected_video_ids, desc=\"Processing videos\"):\n","        video_info = get_video_info(youtube, video_id)\n","        if video_info:\n","            channel_id = video_info['snippet']['channelId']\n","            channel_author = video_info['snippet']['channelTitle']\n","            title = video_info['snippet']['title']\n","            duration = video_info['contentDetails']['duration']\n","            upload_date = format_datetime(video_info['snippet']['publishedAt'])\n","\n","            # 동영상 기본 정보 추가\n","            combined_data.append({\n","                'Channel_ID': channel_id,\n","                'Channel_Title': channel_author,\n","                'Video_ID': video_id,\n","                'Title': title,\n","                'Upload_Date': upload_date,\n","                'Comment_ID': None,\n","                'Comment_Author': None,\n","                'Is_Channel_Author': True,\n","                'Comment_Published_At': None,\n","                'Is_Short': is_short(duration)\n","            })\n","\n","            # 댓글 데이터를 추가\n","            comments = get_all_comments(youtube, video_id)\n","            for comment in comments:\n","                combined_data.append({\n","                    'Channel_ID': channel_id,\n","                    'Channel_Title': channel_author,\n","                    'Video_ID': video_id,\n","                    'Title': title,\n","                    'Upload_Date': upload_date,\n","                    'Comment_ID': comment['comment_thread_id'],\n","                    'Comment_Author': comment['author_display_name'],\n","                    'Is_Channel_Author': (comment['author_channel_id'] == channel_id),\n","                    'Comment_Published_At': comment['published_at'],\n","                    'Is_Short': is_short(duration)\n","                })\n","\n","    return combined_data\n","\n","# 데이터 저장 함수\n","def save_data(data, file_name):\n","    try:\n","        df_combined = pd.DataFrame(data)\n","        df_combined.to_csv(f'{file_name}.csv', index=False)\n","        print(\"데이터를 저장했습니다.\")\n","    except Exception as e:\n","        print(f\"Error saving data: {e}\")\n","\n","# 데이터 전처리 함수\n","def processing1(data):\n","    # 'Upload_Date'와 'Comment_Published_At'을 datetime 형식으로 변환합니다.\n","    data['Upload_Date'] = pd.to_datetime(data['Upload_Date'])\n","    data['Comment_Published_At'] = pd.to_datetime(data['Comment_Published_At'])\n","\n","    # 댓글 ID 기준 결측치를 제거합니다.\n","    data = data.dropna(subset=['Comment_ID'])\n","\n","    # 댓글 ID 기준 중복을 제거합니다.\n","    data = data.drop_duplicates(subset=['Comment_ID'])\n","\n","    # 채널 주인의 댓글을 제외합니다.\n","    data_filtered = data[data['Is_Channel_Author'] != 1]\n","\n","    # 영상별로 동일한 작성자의 댓글을 1개씩만 남깁니다.\n","    data = data_filtered.drop_duplicates(subset=['Video_ID', 'Comment_Author'])\n","\n","    # 인덱스를 리셋합니다.\n","    data = data.reset_index(drop=True)\n","\n","    # 남은 댓글 ID의 개수를 확인합니다.\n","    remaining_comment_count = data['Comment_ID'].nunique()\n","    print(f\"채널 주인의 댓글을 제외하고, 영상별로 작성자의 댓글을 1개씩만 남긴 후의 댓글 ID 개수는 {remaining_comment_count}개 입니다.\")\n","\n","    # 필요한 컬럼만 추출\n","    columns_to_use = [\n","        'Channel_ID', 'Channel_Title', 'Upload_Date', 'Comment_Published_At',\n","        'Video_ID', 'Comment_ID'\n","    ]\n","    data = data[columns_to_use].copy()  # 명시적으로 사본을 만듭니다.\n","\n","    # 날짜 형식에서 년-월-일 정보만 추출\n","    data['Upload_Date'] = data['Upload_Date'].astype(str).str[:10]\n","    data['Comment_Published_At'] = data['Comment_Published_At'].astype(str).str[:10]\n","\n","    # 'Upload_Date'와 'Comment_Published_At'을 datetime 형식으로 변환 (년, 월, 일까지만)\n","    data['Upload_Date'] = pd.to_datetime(data['Upload_Date'], format='%Y-%m-%d', errors='coerce')\n","    data['Comment_Published_At'] = pd.to_datetime(data['Comment_Published_At'], format='%Y-%m-%d', errors='coerce')\n","\n","    # 결과를 저장할 데이터프레임 생성\n","    df = pd.DataFrame()\n","\n","    # 각 채널별로 처리\n","    try:\n","        for channel_id, channel_data in data.groupby('Channel_ID'):\n","            # 첫 업로드 날짜 찾기\n","            first_upload_date = channel_data['Upload_Date'].min()\n","\n","            # 첫 업로드 날짜부터 현재까지의 월요일 기준 주 단위 날짜 생성\n","            date_range = pd.date_range(start=first_upload_date, end=channel_data['Upload_Date'].max() + pd.Timedelta(days=7), freq='W-MON')\n","\n","            # 주간 데이터 저장 리스트 초기화\n","            weekly_data = []\n","\n","            # 누적 값 초기화\n","            cumulative_videos = 0\n","            cumulative_comments = 0\n","\n","            for i, week_start in enumerate(date_range):\n","                week_end = week_start + pd.Timedelta(days=7)\n","\n","                # 현재 주의 비디오와 댓글 필터링\n","                weekly_videos = channel_data[(channel_data['Upload_Date'] >= week_start) & (channel_data['Upload_Date'] < week_end)]\n","                weekly_comments = channel_data[(channel_data['Comment_Published_At'] >= week_start) & (channel_data['Comment_Published_At'] < week_end)]\n","\n","                # 고유 Comment_ID와 Comment_Published_At을 사용하여 누적 값 업데이트\n","                unique_comments = weekly_comments.drop_duplicates(subset=['Comment_ID', 'Comment_Published_At']).shape[0]\n","                cumulative_videos += weekly_videos['Video_ID'].nunique()\n","                cumulative_comments += unique_comments\n","\n","                # 주간 데이터 추가\n","                weekly_data.append({\n","                    'Channel_ID': channel_id,\n","                    'Channel_Title': channel_data['Channel_Title'].iloc[0],\n","                    'Comment_Year': week_start.year,\n","                    'Comment_Week': week_start.isocalendar()[1],\n","                    'Cumulative_Videos': int(cumulative_videos),\n","                    'Cumulative_Comments': int(cumulative_comments),\n","                })\n","\n","            # 현재 채널의 결과 데이터프레임 생성\n","            channel_df = pd.DataFrame(weekly_data)\n","\n","            # 결과에 추가\n","            df = pd.concat([df, channel_df], ignore_index=True)\n","    except Exception as e:\n","        print(f\"데이터 처리 중 오류 발생: {e}\")\n","\n","    # 'Cumulative_Videos'가 0인 행 제거\n","    if 'Cumulative_Videos' in df.columns:\n","        df = df[(df['Cumulative_Videos'] != 0)]\n","    else:\n","        print(\"'Cumulative_Videos' 열이 결과 데이터프레임에 존재하지 않습니다.\")\n","\n","    # 열 순서 정리\n","    expected_columns = ['Channel_ID', 'Channel_Title', 'Comment_Year', 'Comment_Week', 'Cumulative_Videos', 'Cumulative_Comments']\n","    missing_columns = [col for col in expected_columns if col not in df.columns]\n","    if missing_columns:\n","        print(f\"다음 열이 결과 데이터프레임에 없습니다: {missing_columns}\")\n","    else:\n","        df = df[expected_columns]\n","\n","    # 누적 댓글 수를 누적 영상 수로 나누기\n","    df['Comments_Per_Video'] = df['Cumulative_Comments'] / df['Cumulative_Videos']\n","    df['Year_Week'] = df['Comment_Year'].astype(str) + '-' + df['Comment_Week'].astype(str)\n","    df = df.drop(['Channel_ID','Comment_Year','Comment_Week','Cumulative_Videos','Cumulative_Comments'],axis=1)\n","\n","    return df\n","\n","# 슬라이딩 윈도우와 기울기 계산 함수\n","def calculate_slopes(data, periods):\n","    if not isinstance(periods, int):\n","        raise ValueError(\"`periods` must be an integer.\")\n","\n","    slopes = []\n","    channels = data['Channel_Title'].unique()\n","\n","    for period in tqdm([periods], desc='Creating sliding windows and calculating slopes'):\n","        for channel in channels:\n","            channel_data = data[data['Channel_Title'] == channel]\n","            if len(channel_data) >= period:\n","                for i in range(len(channel_data) - period + 1):\n","                    window = channel_data.iloc[i:i+period]\n","                    comments_per_video = window['Comments_Per_Video'].values\n","                    year_week = window['Year_Week'].values[-1]\n","\n","                    x = np.arange(len(comments_per_video)).reshape(-1, 1)\n","                    y = comments_per_video\n","\n","                    model = LinearRegression().fit(x, y)\n","                    slope = model.coef_[0]\n","\n","                    slopes.append({\n","                        'Channel_Title': channel,\n","                        'Period': period,\n","                        'Year_Week': year_week,\n","                        'Window': comments_per_video,\n","                        'Slope': slope\n","                    })\n","        print(f\"Period {period}: {len(slopes)} slopes calculated\")\n","\n","    return pd.DataFrame(slopes)\n","\n","# 기울기 계산 후 결과 처리 함수\n","def process_data1(df, video_type):\n","    Channel_name = df['Channel_Title'].unique()\n","    slopes_dict = {}\n","    slopes_list = []\n","\n","    for channel in Channel_name:\n","        channel_data = df[df['Channel_Title'] == channel]\n","        max_slope_row = channel_data[channel_data['Slope'] == channel_data['Slope'].max()]\n","        max_slope = max_slope_row['Slope'].values[0]\n","        max_year_week = max_slope_row['Year_Week'].values[0]\n","        slopes_dict[channel] = (max_slope, max_year_week)\n","        slopes_list.append(max_slope)\n","\n","        print(f'{video_type}의 경우')\n","        print(f'{channel}의 가장 큰 기울기: {max_slope} (기간: {max_year_week})')\n","\n","        if max_slope > 158.5:\n","            print(f'--> {channel}는 떡상 기준에 적합합니다! (기울기: {max_slope} > 158.5)')\n","            print('-------------------------------')\n","\n","def process_data2(df, video_type):\n","    Channel_name = df['Channel_Title'].unique()\n","    slopes_dict = {}\n","    slopes_list = []\n","\n","    for channel in Channel_name:\n","        channel_data = df[df['Channel_Title'] == channel]\n","        max_slope_row = channel_data[channel_data['Slope'] == channel_data['Slope'].max()]\n","        max_slope = max_slope_row['Slope'].values[0]\n","        max_year_week = max_slope_row['Year_Week'].values[0]\n","        slopes_dict[channel] = (max_slope, max_year_week)\n","        slopes_list.append(max_slope)\n","\n","        print(f'{video_type}의 경우')\n","        print(f'{channel}의 가장 큰 기울기: {max_slope} (기간: {max_year_week})')\n","\n","        if max_slope > 238.18:\n","            print(f'--> {channel}는 떡상 기준에 적합합니다! (기울기: {max_slope} > 238.18)')\n","            print('-------------------------------')\n","\n","# 채널 분석 및 시각화 함수\n","def analyze_channel1(df, channel_name):\n","    font_path = 'C:/Windows/Fonts/malgun.ttf'\n","    font_name = font_manager.FontProperties(fname=font_path).get_name()\n","    rc('font', family=font_name)\n","\n","    df1 = df[df.Channel_Title == channel_name].copy()\n","    df1 = df1.reset_index(drop=True)\n","\n","    if len(df1) <= 5:\n","        print(f\"{channel_name} 채널은 아직 분석에 대한 데이터가 쌓이지 않았습니다.\")\n","        return\n","\n","    signal = df1['Comments_Per_Video']\n","\n","    df1['4_Week_Moving_Avg'] = signal.rolling(window=4).mean()\n","    df1['Weekly_Moving_Avg'] = signal.rolling(window=1).mean()\n","    df1['3_Week_Moving_Avg'] = signal.rolling(window=3).mean()\n","    df1['Pct_Change'] = signal.replace(0, np.nan).pct_change().replace(np.nan, 0)\n","\n","    df1_filtered = df1.copy()\n","    df1_filtered['Moving_Avg_Diff'] = df1_filtered['Weekly_Moving_Avg'] - df1_filtered['3_Week_Moving_Avg']\n","\n","    significant_diff_points = df1_filtered[\n","        (df1_filtered['Weekly_Moving_Avg'] > 10) &\n","        (df1_filtered['3_Week_Moving_Avg'] > 10) &\n","        (df1_filtered['Moving_Avg_Diff'] < df1_filtered['3_Week_Moving_Avg'] * -0.005)\n","    ]\n","\n","    num_periods = len(df1_filtered)\n","    recent_periods = int(num_periods * 0.25)\n","    recent_data = df1_filtered.tail(recent_periods)\n","\n","    recent_significant_diff_points = recent_data[\n","        (recent_data['Weekly_Moving_Avg'] > 10) &\n","        (recent_data['3_Week_Moving_Avg'] > 10) &\n","        (recent_data['Moving_Avg_Diff'] < recent_data['3_Week_Moving_Avg'] * -0.005)\n","    ]\n","\n","    if len(recent_significant_diff_points) / recent_periods > 0.30:\n","        trend_status = \"하락 중\"\n","    else:\n","        trend_status = \"유지 중\"\n","\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(df1_filtered['Year_Week'], df1_filtered['Comments_Per_Video'], label='Comments Per Video', marker='o')\n","    plt.plot(df1_filtered['Year_Week'], df1_filtered['4_Week_Moving_Avg'], label='4-Week Moving Average', linestyle='--')\n","    plt.plot(df1_filtered['Year_Week'], df1_filtered['Weekly_Moving_Avg'], label='Weekly Moving Average', linestyle='-.')\n","    plt.plot(df1_filtered['Year_Week'], df1_filtered['3_Week_Moving_Avg'], label='3-Week Moving Average', linestyle=':')\n","\n","    if not significant_diff_points.empty:\n","        plt.plot(significant_diff_points['Year_Week'], significant_diff_points['Comments_Per_Video'], 'ro', label='Significant Difference (<10%)')\n","\n","    plt.xlabel('Year-Week')\n","    plt.ylabel('Comments Per Video')\n","    plt.title(f'{channel_name}: Comments Per Video Analysis')\n","    plt.xticks(rotation=90)\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","\n","    print(f'{channel_name}의 최근 25% 기간 상태: {trend_status}')\n","\n","# 메인 함수\n","def main():\n","    api_key = input(\"API 키를 입력하세요 (종료하려면 'done' 입력): \")\n","    if api_key.lower() == 'done':\n","        print(\"프로그램을 종료합니다.\")\n","        return\n","\n","    youtube = get_youtube_service(api_key)\n","\n","    # 채널명을 입력받아 해당 채널의 비디오 ID를 가져옵니다.\n","    channel_name = input(\"채널명을 입력하세요: \")\n","    channel_id = get_channel_id_by_name(youtube, channel_name)\n","    if not channel_id:\n","        print(f\"채널 '{channel_name}'을 찾을 수 없습니다.\")\n","        return\n","\n","    video_ids = get_video_ids_from_channel(youtube, channel_id)\n","    print(f\"채널 '{channel_name}'에서 {len(video_ids)}개의 비디오를 찾았습니다.\")\n","\n","    start_index = int(input(\"비디오 ID를 가져올 시작 인덱스를 입력하세요 (0부터 시작): \"))\n","    end_index = int(input(\"비디오 ID를 가져올 끝 인덱스를 입력하세요 (마지막 비디오 ID는 포함되지 않음): \"))\n","\n","    file_name = input(\"저장할 파일명을 입력하세요 (확장자 제외): \")\n","\n","    if not file_name:\n","        print(\"파일명은 필수 입력 사항입니다.\")\n","        return\n","\n","    data = collect_data(youtube, video_ids, start_index, end_index)\n","    save_data(data, file_name)\n","\n","    # 전처리\n","    df = pd.read_csv(f'{file_name}.csv')\n","\n","    # 원본 데이터프레임을 복사하여 사용합니다.\n","    data = df.copy()\n","\n","    # 'Is_Short' 값에 따라 데이터프레임을 나누어 복사합니다.\n","    data1 = data[data['Is_Short'] == True].copy()\n","    data2 = data[data['Is_Short'] == False].copy()\n","\n","    # 각 데이터프레임을 처리하고 결과를 저장합니다.\n","    data1_processed = processing1(data1)\n","    data2_processed = processing1(data2)\n","\n","    period = 2\n","    data1_slopes = calculate_slopes(data1_processed, period)\n","    data2_slopes = calculate_slopes(data2_processed, period)\n","\n","    process_data1(data1_slopes, \"쇼츠\")\n","    process_data2(data2_slopes, \"긴 동영상\")\n","\n","    Channel_name = data1_processed['Channel_Title'].unique()\n","\n","    for df in [data1_processed, data2_processed]:\n","        for channel in Channel_name:\n","            analyze_channel1(df, channel)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"W03cbLgGfYcQ"},"execution_count":null,"outputs":[]}]}